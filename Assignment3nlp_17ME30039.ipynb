{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the different datasets given\n",
    "E_train = open(r'D:\\third_year\\nlp\\a1\\Corpus\\eng.train.conllu',encoding=\"utf8\")\n",
    "E_test = open(r'D:\\third_year\\nlp\\a1\\Corpus\\eng.test.conllu',encoding=\"utf8\")\n",
    "S_train = open(r'D:\\third_year\\nlp\\a1\\Corpus\\spanish.train.conllu',encoding=\"utf8\")\n",
    "S_test = open(r'D:\\third_year\\nlp\\a1\\Corpus\\spanish.test.conllu',encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this code is taken from the link given in the assignment\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tempfile\n",
    "import pickle\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from os import remove\n",
    "from copy import deepcopy\n",
    "from operator import itemgetter\n",
    "\n",
    "try:\n",
    "    from numpy import array\n",
    "    from scipy import sparse\n",
    "    from sklearn.datasets import load_svmlight_file\n",
    "    from sklearn import svm\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "from nltk.parse import ParserI, DependencyGraph, DependencyEvaluator\n",
    "\n",
    "\n",
    "class Configuration(object):\n",
    "    \"\"\"\n",
    "    Class for holding configuration which is the partial analysis of the input sentence.\n",
    "    The transition based parser aims at finding set of operators that transfer the initial\n",
    "    configuration to the terminal configuration.\n",
    "\n",
    "    The configuration includes:\n",
    "        - Stack: for storing partially proceeded words\n",
    "        - Buffer: for storing remaining input words\n",
    "        - Set of arcs: for storing partially built dependency tree\n",
    "\n",
    "    This class also provides a method to represent a configuration as list of features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dep_graph):\n",
    "        \"\"\"\n",
    "        :param dep_graph: the representation of an input in the form of dependency graph.\n",
    "        :type dep_graph: DependencyGraph where the dependencies are not specified.\n",
    "        \"\"\"\n",
    "        # dep_graph.nodes contain list of token for a sentence\n",
    "        self.stack = [0]  # The root element\n",
    "        self.buffer = list(range(1, len(dep_graph.nodes)))  # The rest is in the buffer\n",
    "        self.arcs = []  # empty set of arc\n",
    "        self._tokens = dep_graph.nodes\n",
    "        self._max_address = len(self.buffer)\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            'Stack : '\n",
    "            + str(self.stack)\n",
    "            + '  Buffer : '\n",
    "            + str(self.buffer)\n",
    "            + '   Arcs : '\n",
    "            + str(self.arcs)\n",
    "        )\n",
    "\n",
    "    def _check_informative(self, feat, flag=False):\n",
    "        \"\"\"\n",
    "        Check whether a feature is informative\n",
    "        The flag control whether \"_\" is informative or not\n",
    "        \"\"\"\n",
    "        if feat is None:\n",
    "            return False\n",
    "        if feat == '':\n",
    "            return False\n",
    "        if flag is False:\n",
    "            if feat == '_':\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def extract_features(self):\n",
    "        \"\"\"\n",
    "        Extract the set of features for the current configuration. Implement standard features as describe in\n",
    "        Table 3.2 (page 31) in Dependency Parsing book by Sandra Kubler, Ryan McDonal, Joakim Nivre.\n",
    "        Please note that these features are very basic.\n",
    "        :return: list(str)\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # Todo : can come up with more complicated features set for better\n",
    "        # performance.\n",
    "        if len(self.stack) > 0:\n",
    "            # Stack 0\n",
    "            stack_idx0 = self.stack[len(self.stack) - 1]\n",
    "            token = self._tokens[stack_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('STK_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('STK_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('STK_0_POS_' + token['tag'])\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('STK_0_FEATS_' + feat)\n",
    "            # Stack 1\n",
    "            if len(self.stack) > 1:\n",
    "                stack_idx1 = self.stack[len(self.stack) - 2]\n",
    "                token = self._tokens[stack_idx1]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('STK_1_POS_' + token['tag'])\n",
    "\n",
    "            # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == stack_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('STK_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('STK_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        # Check Buffered 0\n",
    "        if len(self.buffer) > 0:\n",
    "            # Buffer 0\n",
    "            buffer_idx0 = self.buffer[0]\n",
    "            token = self._tokens[buffer_idx0]\n",
    "            if self._check_informative(token['word'], True):\n",
    "                result.append('BUF_0_FORM_' + token['word'])\n",
    "            if 'lemma' in token and self._check_informative(token['lemma']):\n",
    "                result.append('BUF_0_LEMMA_' + token['lemma'])\n",
    "            if self._check_informative(token['tag']):\n",
    "                result.append('BUF_0_POS_' + token['tag'])\n",
    "            if 'feats' in token and self._check_informative(token['feats']):\n",
    "                feats = token['feats'].split(\"|\")\n",
    "                for feat in feats:\n",
    "                    result.append('BUF_0_FEATS_' + feat)\n",
    "            # Buffer 1\n",
    "            if len(self.buffer) > 1:\n",
    "                buffer_idx1 = self.buffer[1]\n",
    "                token = self._tokens[buffer_idx1]\n",
    "                if self._check_informative(token['word'], True):\n",
    "                    result.append('BUF_1_FORM_' + token['word'])\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_1_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 2:\n",
    "                buffer_idx2 = self.buffer[2]\n",
    "                token = self._tokens[buffer_idx2]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_2_POS_' + token['tag'])\n",
    "            if len(self.buffer) > 3:\n",
    "                buffer_idx3 = self.buffer[3]\n",
    "                token = self._tokens[buffer_idx3]\n",
    "                if self._check_informative(token['tag']):\n",
    "                    result.append('BUF_3_POS_' + token['tag'])\n",
    "                    # Left most, right most dependency of stack[0]\n",
    "            left_most = 1000000\n",
    "            right_most = -1\n",
    "            dep_left_most = ''\n",
    "            dep_right_most = ''\n",
    "            for (wi, r, wj) in self.arcs:\n",
    "                if wi == buffer_idx0:\n",
    "                    if (wj > wi) and (wj > right_most):\n",
    "                        right_most = wj\n",
    "                        dep_right_most = r\n",
    "                    if (wj < wi) and (wj < left_most):\n",
    "                        left_most = wj\n",
    "                        dep_left_most = r\n",
    "            if self._check_informative(dep_left_most):\n",
    "                result.append('BUF_0_LDEP_' + dep_left_most)\n",
    "            if self._check_informative(dep_right_most):\n",
    "                result.append('BUF_0_RDEP_' + dep_right_most)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "\n",
    "class Transition(object):\n",
    "    \"\"\"\n",
    "    This class defines a set of transition which is applied to a configuration to get another configuration\n",
    "    Note that for different parsing algorithm, the transition is different.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define set of transitions\n",
    "    LEFT_ARC = 'LEFTARC'\n",
    "    RIGHT_ARC = 'RIGHTARC'\n",
    "    SHIFT = 'SHIFT'\n",
    "    REDUCE = 'REDUCE'\n",
    "\n",
    "    def __init__(self, alg_option):\n",
    "        \"\"\"\n",
    "        :param alg_option: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type alg_option: str\n",
    "        \"\"\"\n",
    "        self._algo = alg_option\n",
    "        if alg_option not in [\n",
    "            TransitionParser.ARC_STANDARD,\n",
    "            TransitionParser.ARC_EAGER,\n",
    "        ]:\n",
    "            raise ValueError(\n",
    "                \" Currently we only support %s and %s \"\n",
    "                % (TransitionParser.ARC_STANDARD, TransitionParser.ARC_EAGER)\n",
    "            )\n",
    "\n",
    "    def left_arc(self, conf, relation):\n",
    "        \"\"\"\n",
    "        Note that the algorithm for left-arc is quite similar except for precondition for both arc-standard and arc-eager\n",
    "            :param configuration: is the current configuration\n",
    "            :return : A new configuration or -1 if the pre-condition is not satisfied\n",
    "        \"\"\"\n",
    "        if (len(conf.buffer) <= 0) or (len(conf.stack) <= 0):\n",
    "            return -1\n",
    "        if conf.buffer[0] == 0:\n",
    "            # here is the Root element\n",
    "            return -1\n",
    "\n",
    "        idx_wi = conf.stack[len(conf.stack) - 1]\n",
    "\n",
    "        flag = True\n",
    "        if self._algo == TransitionParser.ARC_EAGER:\n",
    "            for (idx_parent, r, idx_child) in conf.arcs:\n",
    "                if idx_child == idx_wi:\n",
    "                    flag = False\n",
    "\n",
    "        if flag:\n",
    "            conf.stack.pop()\n",
    "            idx_wj = conf.buffer[0]\n",
    "            conf.arcs.append((idx_wj, relation, idx_wi))\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "\n",
    "    def right_arc(self, conf, relation):\n",
    "        \"\"\"\n",
    "        Note that the algorithm for right-arc is DIFFERENT for arc-standard and arc-eager\n",
    "            :param configuration: is the current configuration\n",
    "            :return : A new configuration or -1 if the pre-condition is not satisfied\n",
    "        \"\"\"\n",
    "        if (len(conf.buffer) <= 0) or (len(conf.stack) <= 0):\n",
    "            return -1\n",
    "        if self._algo == TransitionParser.ARC_STANDARD:\n",
    "            idx_wi = conf.stack.pop()\n",
    "            idx_wj = conf.buffer[0]\n",
    "            conf.buffer[0] = idx_wi\n",
    "            conf.arcs.append((idx_wi, relation, idx_wj))\n",
    "        else:  # arc-eager\n",
    "            idx_wi = conf.stack[len(conf.stack) - 1]\n",
    "            idx_wj = conf.buffer.pop(0)\n",
    "            conf.stack.append(idx_wj)\n",
    "            conf.arcs.append((idx_wi, relation, idx_wj))\n",
    "\n",
    "\n",
    "    def reduce(self, conf):\n",
    "        \"\"\"\n",
    "        Note that the algorithm for reduce is only available for arc-eager\n",
    "            :param configuration: is the current configuration\n",
    "            :return : A new configuration or -1 if the pre-condition is not satisfied\n",
    "        \"\"\"\n",
    "\n",
    "        if self._algo != TransitionParser.ARC_EAGER:\n",
    "            return -1\n",
    "        if len(conf.stack) <= 0:\n",
    "            return -1\n",
    "\n",
    "        idx_wi = conf.stack[len(conf.stack) - 1]\n",
    "        flag = False\n",
    "        for (idx_parent, r, idx_child) in conf.arcs:\n",
    "            if idx_child == idx_wi:\n",
    "                flag = True\n",
    "        if flag:\n",
    "            conf.stack.pop()  # reduce it\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "\n",
    "    def shift(self, conf):\n",
    "        \"\"\"\n",
    "        Note that the algorithm for shift is the SAME for arc-standard and arc-eager\n",
    "            :param configuration: is the current configuration\n",
    "            :return : A new configuration or -1 if the pre-condition is not satisfied\n",
    "        \"\"\"\n",
    "        if len(conf.buffer) <= 0:\n",
    "            return -1\n",
    "        idx_wi = conf.buffer.pop(0)\n",
    "        conf.stack.append(idx_wi)\n",
    "\n",
    "\n",
    "\n",
    "class TransitionParser(ParserI):\n",
    "\n",
    "    \"\"\"\n",
    "    Class for transition based parser. Implement 2 algorithms which are \"arc-standard\" and \"arc-eager\"\n",
    "    \"\"\"\n",
    "\n",
    "    ARC_STANDARD = 'arc-standard'\n",
    "    ARC_EAGER = 'arc-eager'\n",
    "\n",
    "    def __init__(self, algorithm):\n",
    "        \"\"\"\n",
    "        :param algorithm: the algorithm option of this parser. Currently support `arc-standard` and `arc-eager` algorithm\n",
    "        :type algorithm: str\n",
    "        \"\"\"\n",
    "        if not (algorithm in [self.ARC_STANDARD, self.ARC_EAGER]):\n",
    "            raise ValueError(\n",
    "                \" Currently we only support %s and %s \"\n",
    "                % (self.ARC_STANDARD, self.ARC_EAGER)\n",
    "            )\n",
    "        self._algorithm = algorithm\n",
    "\n",
    "        self._dictionary = {}\n",
    "        self._transition = {}\n",
    "        self._match_transition = {}\n",
    "\n",
    "    def _get_dep_relation(self, idx_parent, idx_child, depgraph):\n",
    "        p_node = depgraph.nodes[idx_parent]\n",
    "        c_node = depgraph.nodes[idx_child]\n",
    "\n",
    "        if c_node['word'] is None:\n",
    "            return None  # Root word\n",
    "\n",
    "        if c_node['head'] == p_node['address']:\n",
    "            return c_node['rel']\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _convert_to_binary_features(self, features):\n",
    "        \"\"\"\n",
    "        :param features: list of feature string which is needed to convert to binary features\n",
    "        :type features: list(str)\n",
    "        :return : string of binary features in libsvm format  which is 'featureID:value' pairs\n",
    "        \"\"\"\n",
    "        unsorted_result = []\n",
    "        for feature in features:\n",
    "            self._dictionary.setdefault(feature, len(self._dictionary))\n",
    "            unsorted_result.append(self._dictionary[feature])\n",
    "\n",
    "        # Default value of each feature is 1.0\n",
    "        return ' '.join(\n",
    "            str(featureID) + ':1.0' for featureID in sorted(unsorted_result)\n",
    "        )\n",
    "\n",
    "    def _is_projective(self, depgraph):\n",
    "        arc_list = []\n",
    "        for key in depgraph.nodes:\n",
    "            node = depgraph.nodes[key]\n",
    "\n",
    "            if 'head' in node:\n",
    "                childIdx = node['address']\n",
    "                parentIdx = node['head']\n",
    "                if parentIdx is not None:\n",
    "                    arc_list.append((parentIdx, childIdx))\n",
    "\n",
    "        for (parentIdx, childIdx) in arc_list:\n",
    "            # Ensure that childIdx < parentIdx\n",
    "            if childIdx > parentIdx:\n",
    "                temp = childIdx\n",
    "                childIdx = parentIdx\n",
    "                parentIdx = temp\n",
    "            for k in range(childIdx + 1, parentIdx):\n",
    "                for m in range(len(depgraph.nodes)):\n",
    "                    if (m < childIdx) or (m > parentIdx):\n",
    "                        if (k, m) in arc_list:\n",
    "                            return False\n",
    "                        if (m, k) in arc_list:\n",
    "                            return False\n",
    "        return True\n",
    "\n",
    "    def _write_to_file(self, key, binary_features, input_file):\n",
    "        \"\"\"\n",
    "        write the binary features to input file and update the transition dictionary\n",
    "        \"\"\"\n",
    "        self._transition.setdefault(key, len(self._transition) + 1)\n",
    "        self._match_transition[self._transition[key]] = key\n",
    "\n",
    "        input_str = str(self._transition[key]) + ' ' + binary_features + '\\n'\n",
    "        input_file.write(input_str.encode('utf-8'))\n",
    "\n",
    "    def _create_training_examples_arc_std(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : Page 32, Chapter 3. Dependency Parsing by Sandra Kubler, Ryan McDonal and Joakim Nivre (2009)\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_STANDARD)\n",
    "        count_proj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            count_proj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        precondition = True\n",
    "                        # Get the max-index of buffer\n",
    "                        maxID = conf._max_address\n",
    "\n",
    "                        for w in range(maxID + 1):\n",
    "                            if w != b0:\n",
    "                                relw = self._get_dep_relation(b0, w, depgraph)\n",
    "                                if relw is not None:\n",
    "                                    if (b0, relw, w) not in conf.arcs:\n",
    "                                        precondition = False\n",
    "\n",
    "                        if precondition:\n",
    "                            key = Transition.RIGHT_ARC + ':' + rel\n",
    "                            self._write_to_file(key, binary_features, input_file)\n",
    "                            operation.right_arc(conf, rel)\n",
    "                            training_seq.append(key)\n",
    "                            continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(count_proj))\n",
    "        return training_seq\n",
    "\n",
    "    def _create_training_examples_arc_eager(self, depgraphs, input_file):\n",
    "        \"\"\"\n",
    "        Create the training example in the libsvm format and write it to the input_file.\n",
    "        Reference : 'A Dynamic Oracle for Arc-Eager Dependency Parsing' by Joav Goldberg and Joakim Nivre\n",
    "        \"\"\"\n",
    "        operation = Transition(self.ARC_EAGER)\n",
    "        countProj = 0\n",
    "        training_seq = []\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            if not self._is_projective(depgraph):\n",
    "                continue\n",
    "\n",
    "            countProj += 1\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                b0 = conf.buffer[0]\n",
    "                features = conf.extract_features()\n",
    "                binary_features = self._convert_to_binary_features(features)\n",
    "\n",
    "                if len(conf.stack) > 0:\n",
    "                    s0 = conf.stack[len(conf.stack) - 1]\n",
    "                    # Left-arc operation\n",
    "                    rel = self._get_dep_relation(b0, s0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.LEFT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.left_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # Right-arc operation\n",
    "                    rel = self._get_dep_relation(s0, b0, depgraph)\n",
    "                    if rel is not None:\n",
    "                        key = Transition.RIGHT_ARC + ':' + rel\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.right_arc(conf, rel)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                    # reduce operation\n",
    "                    flag = False\n",
    "                    for k in range(s0):\n",
    "                        if self._get_dep_relation(k, b0, depgraph) is not None:\n",
    "                            flag = True\n",
    "                        if self._get_dep_relation(b0, k, depgraph) is not None:\n",
    "                            flag = True\n",
    "                    if flag:\n",
    "                        key = Transition.REDUCE\n",
    "                        self._write_to_file(key, binary_features, input_file)\n",
    "                        operation.reduce(conf)\n",
    "                        training_seq.append(key)\n",
    "                        continue\n",
    "\n",
    "                # Shift operation as the default\n",
    "                key = Transition.SHIFT\n",
    "                self._write_to_file(key, binary_features, input_file)\n",
    "                operation.shift(conf)\n",
    "                training_seq.append(key)\n",
    "\n",
    "        print(\" Number of training examples : \" + str(len(depgraphs)))\n",
    "        print(\" Number of valid (projective) examples : \" + str(countProj))\n",
    "        return training_seq\n",
    "\n",
    "    def train(self, depgraphs, modelfile, verbose=True):\n",
    "        \"\"\"\n",
    "        :param depgraphs : list of DependencyGraph as the training data\n",
    "        :type depgraphs : DependencyGraph\n",
    "        :param modelfile : file name to save the trained model\n",
    "        :type modelfile : str\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            input_file = tempfile.NamedTemporaryFile(\n",
    "                prefix='transition_parse.train', dir=tempfile.gettempdir(), delete=False\n",
    "            )\n",
    "\n",
    "            if self._algorithm == self.ARC_STANDARD:\n",
    "                self._create_training_examples_arc_std(depgraphs, input_file)\n",
    "            else:\n",
    "                self._create_training_examples_arc_eager(depgraphs, input_file)\n",
    "\n",
    "            input_file.close()\n",
    "            ## by default nltk uses the svm classifier\n",
    "            # Using the temporary file to train the libsvm classifier\n",
    "            x_train, y_train = load_svmlight_file(input_file.name)\n",
    "            # The parameter is set according to the paper:\n",
    "            # Algorithms for Deterministic Incremental Dependency Parsing by Joakim Nivre\n",
    "            # Todo : because of probability = True => very slow due to\n",
    "            # cross-validation. Need to improve the speed here\n",
    "#             model = svm.SVC(\n",
    "#                 kernel='poly',\n",
    "#                 degree=2,\n",
    "#                 coef0=0,\n",
    "#                 gamma=0.2,\n",
    "#                 C=0.5,\n",
    "#                 verbose=verbose,\n",
    "#                 probability=True,\n",
    "#             )\n",
    "# used Logistic Regression, MLP classifier,Xgboost Decision Tree Classifier\n",
    "#             model = LogisticRegression(random_state=0, solver='lbfgs',max_iter=250,\n",
    "#                         multi_class='multinomial')\n",
    "#             model = xgb.XGBClassifier(booster='gbtree', colsample_bylevel=1,\n",
    "#     colsample_bytree=1, gamma=0, learning_rate=0.15,\n",
    "#     max_delta_step=0, max_depth=3, min_child_weight=1, missing=None,\n",
    "#     n_estimators=700, n_jobs=3, nthread=1, objective='multi:softmax',\n",
    "#     random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "#     seed=None, subsample=1)\n",
    "#             model=xgb.XGBRegressor(objective=\"reg:linear\", random_state=42)\n",
    "            \n",
    "            model = MLPClassifier(solver='lbfgs', alpha=1e-5, random_state=1)\n",
    "#             model = DecisionTreeClassifier(random_state=0)\n",
    "            model.fit(x_train, y_train)\n",
    "            # Save the model to file name (as pickle)\n",
    "            pickle.dump(model, open(modelfile, 'wb'))\n",
    "        finally:\n",
    "            remove(input_file.name)\n",
    "\n",
    "\n",
    "    def parse(self, depgraphs, modelFile):\n",
    "        \"\"\"\n",
    "        :param depgraphs: the list of test sentence, each sentence is represented as a dependency graph where the 'head' information is dummy\n",
    "        :type depgraphs: list(DependencyGraph)\n",
    "        :param modelfile: the model file\n",
    "        :type modelfile: str\n",
    "        :return: list (DependencyGraph) with the 'head' and 'rel' information\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        # First load the model\n",
    "        model = pickle.load(open(modelFile, 'rb'))\n",
    "        operation = Transition(self._algorithm)\n",
    "\n",
    "        for depgraph in depgraphs:\n",
    "            conf = Configuration(depgraph)\n",
    "            while len(conf.buffer) > 0:\n",
    "                features = conf.extract_features()\n",
    "                col = []\n",
    "                row = []\n",
    "                data = []\n",
    "                for feature in features:\n",
    "                    if feature in self._dictionary:\n",
    "                        col.append(self._dictionary[feature])\n",
    "                        row.append(0)\n",
    "                        data.append(1.0)\n",
    "                np_col = array(sorted(col))  # NB : index must be sorted\n",
    "                np_row = array(row)\n",
    "                np_data = array(data)\n",
    "\n",
    "                x_test = sparse.csr_matrix(\n",
    "                    (np_data, (np_row, np_col)), shape=(1, len(self._dictionary))\n",
    "                )\n",
    "\n",
    "                # It's best to use decision function as follow BUT it's not supported yet for sparse SVM\n",
    "                # Using decision funcion to build the votes array\n",
    "                # dec_func = model.decision_function(x_test)[0]\n",
    "                # votes = {}\n",
    "                # k = 0\n",
    "                # for i in range(len(model.classes_)):\n",
    "                #    for j in range(i+1, len(model.classes_)):\n",
    "                #        #if  dec_func[k] > 0:\n",
    "                #            votes.setdefault(i,0)\n",
    "                #            votes[i] +=1\n",
    "                #        else:\n",
    "                #           votes.setdefault(j,0)\n",
    "                #           votes[j] +=1\n",
    "                #        k +=1\n",
    "                # Sort votes according to the values\n",
    "                # sorted_votes = sorted(votes.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # We will use predict_proba instead of decision_function\n",
    "                prob_dict = {}\n",
    "                pred_prob = model.predict_proba(x_test)[0]\n",
    "                for i in range(len(pred_prob)):\n",
    "                    prob_dict[i] = pred_prob[i]\n",
    "                sorted_Prob = sorted(prob_dict.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "                # Note that SHIFT is always a valid operation\n",
    "                for (y_pred_idx, confidence) in sorted_Prob:\n",
    "                    # y_pred = model.predict(x_test)[0]\n",
    "                    # From the prediction match to the operation\n",
    "                    y_pred = model.classes_[y_pred_idx]\n",
    "\n",
    "                    if y_pred in self._match_transition:\n",
    "                        strTransition = self._match_transition[y_pred]\n",
    "                        baseTransition = strTransition.split(\":\")[0]\n",
    "\n",
    "                        if baseTransition == Transition.LEFT_ARC:\n",
    "                            if (\n",
    "                                operation.left_arc(conf, strTransition.split(\":\")[1])\n",
    "                                != -1\n",
    "                            ):\n",
    "                                break\n",
    "                        elif baseTransition == Transition.RIGHT_ARC:\n",
    "                            if (\n",
    "                                operation.right_arc(conf, strTransition.split(\":\")[1])\n",
    "                                != -1\n",
    "                            ):\n",
    "                                break\n",
    "                        elif baseTransition == Transition.REDUCE:\n",
    "                            if operation.reduce(conf) != -1:\n",
    "                                break\n",
    "                        elif baseTransition == Transition.SHIFT:\n",
    "                            if operation.shift(conf) != -1:\n",
    "                                break\n",
    "                    else:\n",
    "                        raise ValueError(\n",
    "                            \"The predicted transition is not recognized, expected errors\"\n",
    "                        )\n",
    "\n",
    "            # Finish with operations build the dependency graph from Conf.arcs\n",
    "\n",
    "            new_depgraph = deepcopy(depgraph)\n",
    "            for key in new_depgraph.nodes:\n",
    "                node = new_depgraph.nodes[key]\n",
    "                node['rel'] = ''\n",
    "                # With the default, all the token depend on the Root\n",
    "                node['head'] = 0\n",
    "            for (head, rel, child) in conf.arcs:\n",
    "                c_node = new_depgraph.nodes[child]\n",
    "                c_node['head'] = head\n",
    "                c_node['rel'] = rel\n",
    "            result.append(new_depgraph)\n",
    "\n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = []\n",
    "t = []\n",
    "## Now we will load the training dataset into a list and we will not read the lines starting with #\n",
    "for x in S_train:\n",
    "    \n",
    "    if (x!='\\n'):\n",
    "        g=x.split()[0]\n",
    "        if g=='#':\n",
    "            continue\n",
    "        t.append(x)\n",
    "    else:\n",
    "        cor.append(t)\n",
    "        t = []\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL PC\\Anaconda3\\lib\\site-packages\\nltk\\parse\\dependencygraph.py:380: UserWarning: The graph doesn't contain a node that depends on the root element.\n",
      "  \"The graph doesn't contain a node \"\n"
     ]
    }
   ],
   "source": [
    "train_graph1 = []\n",
    "for i in range(len(cor)):\n",
    "    if i==412: ## in one of the datasets, data in line number 412 was ambiguous so we skip that particular line\n",
    "        continue\n",
    "    train_graph2 = DependencyGraph(cor[i])\n",
    "    train_graph1.append(train_graph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_std = TransitionParser('arc-standard') ## parser_std will use arc-standard algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os\n",
    "input_file = tempfile.NamedTemporaryFile(prefix = 'transition_parse.train', dir = tempfile.gettempdir(), delete = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "a = []\n",
    "\n",
    "for x in S_test:\n",
    "    if x!='\\n':\n",
    "        if x.split()[0] != '#':\n",
    "            a.append(x)\n",
    "    else:\n",
    "        test.append(a)\n",
    "        a = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL PC\\Anaconda3\\lib\\site-packages\\nltk\\parse\\dependencygraph.py:380: UserWarning: The graph doesn't contain a node that depends on the root element.\n",
      "  \"The graph doesn't contain a node \"\n"
     ]
    }
   ],
   "source": [
    "train_graph2 = []\n",
    "for i in range(len(test)):\n",
    "    if i==106: ## in one of the test data , Line number 106 was ambiguous so we skipped it\n",
    "        continue\n",
    "    t = DependencyGraph(test[i])\n",
    "    train_graph2.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<DependencyGraph with 24 nodes>,\n",
       " <DependencyGraph with 17 nodes>,\n",
       " <DependencyGraph with 26 nodes>,\n",
       " <DependencyGraph with 14 nodes>,\n",
       " <DependencyGraph with 40 nodes>,\n",
       " <DependencyGraph with 8 nodes>,\n",
       " <DependencyGraph with 36 nodes>,\n",
       " <DependencyGraph with 22 nodes>,\n",
       " <DependencyGraph with 31 nodes>,\n",
       " <DependencyGraph with 28 nodes>,\n",
       " <DependencyGraph with 25 nodes>,\n",
       " <DependencyGraph with 18 nodes>,\n",
       " <DependencyGraph with 11 nodes>,\n",
       " <DependencyGraph with 29 nodes>,\n",
       " <DependencyGraph with 43 nodes>,\n",
       " <DependencyGraph with 44 nodes>,\n",
       " <DependencyGraph with 9 nodes>,\n",
       " <DependencyGraph with 29 nodes>,\n",
       " <DependencyGraph with 35 nodes>,\n",
       " <DependencyGraph with 21 nodes>,\n",
       " <DependencyGraph with 28 nodes>,\n",
       " <DependencyGraph with 14 nodes>,\n",
       " <DependencyGraph with 30 nodes>,\n",
       " <DependencyGraph with 30 nodes>,\n",
       " <DependencyGraph with 29 nodes>,\n",
       " <DependencyGraph with 31 nodes>,\n",
       " <DependencyGraph with 14 nodes>,\n",
       " <DependencyGraph with 43 nodes>,\n",
       " <DependencyGraph with 21 nodes>,\n",
       " <DependencyGraph with 17 nodes>,\n",
       " <DependencyGraph with 23 nodes>,\n",
       " <DependencyGraph with 24 nodes>,\n",
       " <DependencyGraph with 17 nodes>,\n",
       " <DependencyGraph with 18 nodes>,\n",
       " <DependencyGraph with 14 nodes>,\n",
       " <DependencyGraph with 23 nodes>,\n",
       " <DependencyGraph with 64 nodes>,\n",
       " <DependencyGraph with 8 nodes>,\n",
       " <DependencyGraph with 48 nodes>,\n",
       " <DependencyGraph with 27 nodes>,\n",
       " <DependencyGraph with 26 nodes>,\n",
       " <DependencyGraph with 19 nodes>,\n",
       " <DependencyGraph with 11 nodes>,\n",
       " <DependencyGraph with 15 nodes>,\n",
       " <DependencyGraph with 14 nodes>,\n",
       " <DependencyGraph with 43 nodes>,\n",
       " <DependencyGraph with 22 nodes>,\n",
       " <DependencyGraph with 35 nodes>,\n",
       " <DependencyGraph with 17 nodes>,\n",
       " <DependencyGraph with 19 nodes>,\n",
       " <DependencyGraph with 75 nodes>,\n",
       " <DependencyGraph with 26 nodes>,\n",
       " <DependencyGraph with 32 nodes>,\n",
       " <DependencyGraph with 12 nodes>,\n",
       " <DependencyGraph with 31 nodes>,\n",
       " <DependencyGraph with 17 nodes>,\n",
       " <DependencyGraph with 14 nodes>,\n",
       " <DependencyGraph with 27 nodes>,\n",
       " <DependencyGraph with 36 nodes>,\n",
       " <DependencyGraph with 6 nodes>,\n",
       " <DependencyGraph with 41 nodes>,\n",
       " <DependencyGraph with 24 nodes>,\n",
       " <DependencyGraph with 44 nodes>,\n",
       " <DependencyGraph with 44 nodes>,\n",
       " <DependencyGraph with 15 nodes>,\n",
       " <DependencyGraph with 28 nodes>,\n",
       " <DependencyGraph with 21 nodes>,\n",
       " <DependencyGraph with 52 nodes>,\n",
       " <DependencyGraph with 8 nodes>,\n",
       " <DependencyGraph with 14 nodes>,\n",
       " <DependencyGraph with 24 nodes>,\n",
       " <DependencyGraph with 9 nodes>,\n",
       " <DependencyGraph with 38 nodes>,\n",
       " <DependencyGraph with 21 nodes>,\n",
       " <DependencyGraph with 27 nodes>,\n",
       " <DependencyGraph with 48 nodes>,\n",
       " <DependencyGraph with 22 nodes>,\n",
       " <DependencyGraph with 46 nodes>,\n",
       " <DependencyGraph with 21 nodes>,\n",
       " <DependencyGraph with 64 nodes>,\n",
       " <DependencyGraph with 22 nodes>,\n",
       " <DependencyGraph with 65 nodes>,\n",
       " <DependencyGraph with 46 nodes>,\n",
       " <DependencyGraph with 30 nodes>,\n",
       " <DependencyGraph with 50 nodes>,\n",
       " <DependencyGraph with 58 nodes>,\n",
       " <DependencyGraph with 13 nodes>,\n",
       " <DependencyGraph with 20 nodes>,\n",
       " <DependencyGraph with 17 nodes>,\n",
       " <DependencyGraph with 13 nodes>,\n",
       " <DependencyGraph with 37 nodes>,\n",
       " <DependencyGraph with 23 nodes>,\n",
       " <DependencyGraph with 41 nodes>,\n",
       " <DependencyGraph with 16 nodes>,\n",
       " <DependencyGraph with 13 nodes>,\n",
       " <DependencyGraph with 13 nodes>,\n",
       " <DependencyGraph with 30 nodes>,\n",
       " <DependencyGraph with 17 nodes>,\n",
       " <DependencyGraph with 8 nodes>,\n",
       " <DependencyGraph with 26 nodes>,\n",
       " <DependencyGraph with 21 nodes>,\n",
       " <DependencyGraph with 17 nodes>,\n",
       " <DependencyGraph with 23 nodes>,\n",
       " <DependencyGraph with 41 nodes>,\n",
       " <DependencyGraph with 56 nodes>,\n",
       " <DependencyGraph with 33 nodes>,\n",
       " <DependencyGraph with 17 nodes>,\n",
       " <DependencyGraph with 26 nodes>,\n",
       " <DependencyGraph with 14 nodes>,\n",
       " <DependencyGraph with 40 nodes>,\n",
       " <DependencyGraph with 8 nodes>,\n",
       " <DependencyGraph with 36 nodes>,\n",
       " <DependencyGraph with 22 nodes>,\n",
       " <DependencyGraph with 31 nodes>,\n",
       " <DependencyGraph with 28 nodes>,\n",
       " <DependencyGraph with 25 nodes>,\n",
       " <DependencyGraph with 18 nodes>]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_graph2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 245\n",
      " Number of valid (projective) examples : 232\n"
     ]
    }
   ],
   "source": [
    "parser_std.train(train_graph1,'temp.arcstd.model', verbose=False) ## training the arc standard model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7311233885819521, 0.6692449355432781)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = parser_std.parse(train_graph2, 'temp.arcstd.model')\n",
    "de = DependencyEvaluator(result, train_graph2)\n",
    "de.eval() # las and uas score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser_std2 = TransitionParser('arc-eager') \n",
    "import tempfile\n",
    "import os\n",
    "input_file = tempfile.NamedTemporaryFile(prefix = 'transition_parse.train', dir = tempfile.gettempdir(), delete = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of training examples : 245\n",
      " Number of valid (projective) examples : 232\n"
     ]
    }
   ],
   "source": [
    "parser_std2.train(train_graph1,'temp.arceager.model', verbose=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7488029465930018, 0.6843462246777164)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = parser_std2.parse(train_graph2, 'temp.arceager.model')\n",
    "de = DependencyEvaluator(result, train_graph2)\n",
    "de.eval() ## it displays the las and uas score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
